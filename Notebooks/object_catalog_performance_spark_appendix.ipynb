{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix: summary of performance using Apache Spark (Apache Parquet and FITS)\n",
    "\n",
    "Author: **Julien Peloton [@JulienPeloton](https://github.com/JulienPeloton)**  \n",
    "Last Run: **2018-11-22**  \n",
    "See also: [issue/249](https://github.com/LSSTDESC/DC2-production/issues/249)\n",
    "\n",
    "This notebook summarises the performance of data manipulations of the DC2 object catalogs with Apache Spark.\n",
    "We focus on Parquet and FITS format.\n",
    "\n",
    "The notebook is intended to be executed with the `desc-pyspark` kernel with **32 threads** (one full Cori Haswell node). See [LSSTDESC/desc-spark](https://github.com/LSSTDESC/desc-spark#working-at-nersc-jupyterlab) for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialise our Spark session\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "We focus on \"One Tract\" (OT) and \"All Tract\" (AT) catalogs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "base_dir = '/global/projecta/projectdirs/lsst/global/in2p3/Run1.1/summary'\n",
    "print(\"Data will be read from: \\n\", base_dir)\n",
    "\n",
    "# Path to data\n",
    "parq_4850_OT = os.path.join(base_dir, 'dpdd_object_tract_4850_hive.parquet')\n",
    "fits_4850_OT = os.path.join(base_dir, 'dpdd_object_tract_4850*.fits')\n",
    "\n",
    "parq_hive_AT = os.path.join(base_dir, 'dpdd_object.parquet')\n",
    "parq_simp_AT = os.path.join(base_dir, 'dpdd_object_simple.parquet')\n",
    "fits_simp_AT = os.path.join(base_dir, 'dpdd_object_tract_*.fits')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "Here is a summary of the results. The configuration for this run was:\n",
    "\n",
    "- \"One Tract\" (OT) and \"All Tract\" (AT) catalogs\n",
    "- 1 full Cori compute node (32 cores).\n",
    "\n",
    "\n",
    "Numbers should be read as order of magnitude (it will slightly vary from run to run, depending also on the load of the machine as JupyterLab is a shared resource).\n",
    "Details can be found below.\n",
    "\n",
    "No cache:\n",
    "\n",
    "| Data set            | #Rows (size GB) | Load time | statistics* |\n",
    "|---------------------|-----------------|-------------|-----------------|\n",
    "| Parquet (OT)        | 719,228 (0.43)   | 192 ms ± 66.5 ms      |      189 ms ± 84 ms     |\n",
    "| FITS (OT)           | 719,228 (0.57)   | 3.74 s ± 107 ms      |      3.67 s ± 123 ms     |\n",
    "| Parquet (AT, Hive)  | 6,892,380 (4.5) | 726 ms ± 117 ms      |      990 ms ± 521 ms     |\n",
    "| Parquet (AT, Simple)| 6,892,380 (3.6) | 210 ms ± 28.2 ms      |      459 ms ± 46.5 ms     |\n",
    "| FITS (AT)           | 6,892,380 (5.4) | 25.7 s ± 308 ms      |      24.4 s ± 1.24 s     |\n",
    "\n",
    "_*statistics_ means computing: number of elements, mean, stddev, min, max.\n",
    "\n",
    "With cache (overhead of < 1 second to add to put data in cache):\n",
    "\n",
    "| Data set            | #Rows (size GB) | Load time | statistics |\n",
    "|---------------------|-----------------|-------------|-----------------|\n",
    "| Parquet (OT)        | 719,228 (0.43)   | 393 ms ± 86.2 ms      | 111 ms ± 45.4 ms |\n",
    "| FITS (OT)           | 719,228 (0.57)   | 312 ms ± 59.2 ms      | 99.5 ms ± 49.4 ms |\n",
    "| Parquet (AT, Hive)  | 6,892,380 (4.5) | 215 ms ± 102 ms      | 351 ms ± 150 ms\n",
    "| Parquet (AT, Simple)| 6,892,380 (3.6) | 181 ms ± 74.1 ms      | 391 ms ± 52.9 ms\n",
    "| FITS (AT)           | 6,892,380 (5.4) | 2.78 s ± 1.37 s      | 3.05 s ± 600 ms\n",
    "\n",
    "Remarks:\n",
    "\n",
    "- Results using Parquet are much faster than FITS. Reasons can be for e.g. columnar vs row-based or better implementation of the Spark Parquet connector. I believe this is related to what is seen with Dask between Parquet and HDF5.\n",
    "- Note however that the size on disk of the datasets varies: 4.5 GB for Parquet Hive, 3.6 GB for Parquet simple, and 5.4 GB for FITS.\n",
    "- For FITS, the number of input files matter. It is always better to have small number of large files rather than many small files.\n",
    "- Once data in cache, everything is super fast.\n",
    "- Note that given the small volume of data, most of the results below the seconds are basically dominated by Spark noise and not actual computation (which is why sometimes a simple count can be slower than computing full statistics)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Details of the benchmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import col\n",
    "import time\n",
    "\n",
    "def readfits(path: str, hdu: int=1) -> DataFrame:\n",
    "    \"\"\" Wrapper around Spark reader for FITS\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    path : str\n",
    "        Path to the data. Can be a file, a folder, or a\n",
    "        glob pattern.\n",
    "    hdu : int, optional\n",
    "        HDU number to read. Default is 1.\n",
    "        \n",
    "    Returns\n",
    "    ----------\n",
    "    df : DataFrame\n",
    "        Spark DataFrame with the HDU data.\n",
    "\n",
    "    \"\"\"\n",
    "    return spark.read.format(\"fits\").option(\"hdu\", hdu).load(path)\n",
    "\n",
    "def readparq(path: str) -> DataFrame:\n",
    "    \"\"\" Wrapper around Spark reader for Parquet\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    path : str\n",
    "        Path to the data. Can be a file, a folder, or a\n",
    "        glob pattern.\n",
    "        \n",
    "    Returns\n",
    "    ----------\n",
    "    df : DataFrame\n",
    "        Spark DataFrame with the HDU data.\n",
    "        \n",
    "    \"\"\"\n",
    "    return spark.read.format(\"parquet\").load(path)\n",
    "\n",
    "def simple_count(df: DataFrame, cache=False, txt: str=\"\") -> int:\n",
    "    \"\"\" Return the number of rows in the DataFrame\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : DataFrame\n",
    "        Spark DataFrame\n",
    "    cache : bool, optional\n",
    "        If True, put the Data in cache prior to the computation.\n",
    "        Data will be unpersisted afterwards. Default is False.\n",
    "    txt: str, optional\n",
    "        Additional text to be printed.\n",
    "        \n",
    "    Returns\n",
    "    ----------\n",
    "    out : int\n",
    "        Number of rows\n",
    "        \n",
    "    \"\"\"\n",
    "    if cache:\n",
    "        start = time.time()\n",
    "        df = df.cache()\n",
    "        print(\"Cache took {:.1f} sec\".format(time.time() - start))\n",
    "    \n",
    "    res = df.count()\n",
    "    print(\"{} has length:\".format(txt), res)\n",
    "    \n",
    "    # Time it!\n",
    "    %timeit df.count()\n",
    "        \n",
    "    return res\n",
    "\n",
    "def stat_diff_col(\n",
    "        df: DataFrame, colname_1: str, colname_2: str, \n",
    "        cache=False, txt: str=\"\") -> DataFrame:\n",
    "    \"\"\" Return some statistics about the difference of \n",
    "    two DataFrame Columns.\n",
    "    Statistics include: count, mean, stddev, min, max.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : DataFrame\n",
    "        Spark DataFrame\n",
    "    colname_1 : str\n",
    "        Name of the first Column\n",
    "    colname_2 : str\n",
    "        Name of the second Column\n",
    "    cache : bool, optional\n",
    "        If True, put the Data in cache prior to the computation.\n",
    "        Data will be unpersisted afterwards. Default is False.\n",
    "    txt: str, optional\n",
    "        Additional text to be printed.\n",
    "        \n",
    "    Returns\n",
    "    ----------\n",
    "    out : DataFrame\n",
    "        DataFrame containing statistics about the Columns difference.\n",
    "    \"\"\"\n",
    "    if cache:\n",
    "        df = df.cache()\n",
    "    print(\"{} has length:\".format(txt), df.count())\n",
    "    \n",
    "    # Time it!\n",
    "    %timeit res = df.select(col(colname_1) - col(colname_2)).describe().collect()\n",
    "    \n",
    "    return df.select(col(colname_1) - col(colname_2)).describe()\n",
    "\n",
    "\n",
    "def stat_one_col(df: DataFrame, colname: str, cache=False, txt: str=\"\") -> DataFrame:\n",
    "    \"\"\" Return some statistics about one DataFrame Column.\n",
    "    Statistics include: count, mean, stddev, min, max.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : DataFrame\n",
    "        Spark DataFrame\n",
    "    colname : str\n",
    "        Name of the Column for which we want the statistics\n",
    "    cache : bool, optional\n",
    "        If True, put the Data in cache prior to the computation.\n",
    "        Data will be unpersisted afterwards. Default is False.\n",
    "    txt: str, optional\n",
    "        Additional text to be printed.\n",
    "        \n",
    "    Returns\n",
    "    ----------\n",
    "    out : DataFrame\n",
    "        DataFrame containing statistics about the Column.\n",
    "    \"\"\"\n",
    "    if cache:\n",
    "        df = df.cache()\n",
    "    print(\"{} has length:\".format(txt), df.count())\n",
    "    \n",
    "    # Time it!\n",
    "    %timeit res = df.select(colname).describe().collect()\n",
    "    \n",
    "    return df.select(colname).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accessing catalogs\n",
    "cache = False\n",
    "df = readparq(parq_4850_OT)\n",
    "c = simple_count(df, cache, \"OT (Parquet)\")\n",
    "\n",
    "df = readfits(fits_4850_OT)\n",
    "c = simple_count(df, cache, \"OT (FITS)\")\n",
    "\n",
    "df = readparq(parq_hive_AT)\n",
    "c = simple_count(df, cache, \"AT (P-Hive)\")\n",
    "\n",
    "df = readparq(parq_simp_AT)\n",
    "c = simple_count(df, cache, \"AT (P-simple)\")\n",
    "\n",
    "df = readfits(fits_simp_AT)\n",
    "c = simple_count(df, cache, \"AT (FITS)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistics: count, mean, stddev, min, max\n",
    "c1 = \"mag_g\"\n",
    "c2 = \"mag_r\"\n",
    "cache = False\n",
    "df = readparq(parq_4850_OT)\n",
    "c = stat_diff_col(df, c1, c2, cache, \"OT (Parquet)\")\n",
    "\n",
    "df = readfits(fits_4850_OT)\n",
    "c = stat_diff_col(df, c1, c2, cache, \"OT (FITS)\")\n",
    "\n",
    "df = readparq(parq_hive_AT)\n",
    "c = stat_diff_col(df, c1, c2, cache, \"AT (P-Hive)\")\n",
    "\n",
    "df = readparq(parq_simp_AT)\n",
    "c = stat_diff_col(df, c1, c2, cache, \"AT (P-simple)\")\n",
    "\n",
    "df = readfits(fits_simp_AT)\n",
    "c = stat_diff_col(df, c1, c2, cache, \"AT (FITS)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accessing catalogs\n",
    "cache = True\n",
    "df = readparq(parq_4850_OT)\n",
    "c = simple_count(df, cache, \"OT (Parquet)\")\n",
    "df.unpersist()\n",
    "\n",
    "df = readfits(fits_4850_OT)\n",
    "c = simple_count(df, cache, \"OT (FITS)\")\n",
    "df.unpersist()\n",
    "\n",
    "df = readparq(parq_hive_AT)\n",
    "c = simple_count(df, cache, \"AT (P-Hive)\")\n",
    "df.unpersist()\n",
    "\n",
    "df = readparq(parq_simp_AT)\n",
    "c = simple_count(df, cache, \"AT (P-simple)\")\n",
    "df.unpersist()\n",
    "\n",
    "df = readfits(fits_simp_AT)\n",
    "c = simple_count(df, cache, \"AT (FITS)\")\n",
    "df.unpersist();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistics: count, mean, stddev, min, max\n",
    "c1 = \"mag_g\"\n",
    "c2 = \"mag_r\"\n",
    "cache = True\n",
    "df = readparq(parq_4850_OT)\n",
    "c = stat_diff_col(df, c1, c2, cache, \"OT (Parquet)\")\n",
    "df.unpersist()\n",
    "\n",
    "df = readfits(fits_4850_OT)\n",
    "c = stat_diff_col(df, c1, c2, cache, \"OT (FITS)\")\n",
    "df.unpersist()\n",
    "\n",
    "df = readparq(parq_hive_AT)\n",
    "c = stat_diff_col(df, c1, c2, cache, \"AT (P-Hive)\")\n",
    "df.unpersist()\n",
    "\n",
    "df = readparq(parq_simp_AT)\n",
    "c = stat_diff_col(df, c1, c2, cache, \"AT (P-simple)\")\n",
    "df.unpersist()\n",
    "\n",
    "df = readfits(fits_simp_AT)\n",
    "c = stat_diff_col(df, c1, c2, cache, \"AT (FITS)\")\n",
    "df.unpersist();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark_2.3.0_dev (2.3.0)",
   "language": "python",
   "name": "pyspark_2.3.0_dev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
